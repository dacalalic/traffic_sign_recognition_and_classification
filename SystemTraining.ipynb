{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate hog descriptors from images that contain traffic signs(positives).\n",
    "def calcHogTrafficSigns(rootpath):\n",
    "    \n",
    "    hog = cv2.HOGDescriptor((64, 64), (16, 16), (8, 8), (8, 8), 9)\n",
    "    features = []\n",
    "    \n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/'\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv')\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')\n",
    "        next(gtReader, None)\n",
    "\n",
    "        for row in gtReader:\n",
    "            resized_image = cv2.resize(cv2.imread(prefix + row[0]), (64, 64))\n",
    "            fd = hog.compute(resized_image)\n",
    "            features.append(np.ravel(fd).tolist())\n",
    "        gtFile.close()\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate hog descriptors from images.\n",
    "def calculateHog(folder):\n",
    "    \n",
    "    hog = cv2.HOGDescriptor((64, 64), (16, 16), (8, 8), (8, 8), 9)\n",
    "    features = []\n",
    "    \n",
    "    #Iterates through all images in the folder.\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        \n",
    "        #Resizes and reshapes the image if needed.\n",
    "        if img.shape[0] != 64 or img.shape[1] != 64:\n",
    "            if img.shape[0] < img.shape[1]:\n",
    "                img = img[0:img.shape[0], 0:img.shape[0]]\n",
    "            else:\n",
    "                img = img[0:img.shape[1], 0:img.shape[1]]\n",
    "                \n",
    "            img = cv2.resize(img, (64, 64))\n",
    "            \n",
    "        fd = hog.compute(img)\n",
    "        features.append(np.ravel(fd).tolist())\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFeaturesToCSV(filename, features):\n",
    "    with open(filename, mode='w', newline='') as features_file:\n",
    "        features_writer = csv.writer(features_file)\n",
    "\n",
    "        for elem in features:\n",
    "            features_writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendFeaturesToCSV(filename, features):\n",
    "    with open(filename, mode='a', newline='') as features_file:\n",
    "        features_writer = csv.writer(features_file)\n",
    "\n",
    "        for elem in features:\n",
    "            features_writer.writerow(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(filename, max_rows=None, delimiter=',', skip_header=0):\n",
    "    return np.genfromtxt(filename, max_rows=max_rows, delimiter=delimiter, skip_header=skip_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGN_FEATURES_FILE_NAME = \"sign_features.csv\"\n",
    "NON_SIGN_FEATURES_FILE_NAME = \"negative_features.csv\"\n",
    "MY_NEGATIVES_FILE_NAME = \"my_negatives.csv\"\n",
    "\n",
    "def getFeaturesAndWrite():\n",
    "    #Acquires the positive features and writes them to the file.\n",
    "    rootpath = \"dataset/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images\"\n",
    "    features = calcHogTrafficSigns(rootpath)\n",
    "    writeFeaturesToCSV(SIGN_FEATURES_FILE_NAME, features)\n",
    "\n",
    "    #Acquires the hard mined negative features and writes them to the file.\n",
    "    rootpath = \"dataset/autogen\"\n",
    "    features = calculateHog(rootpath)\n",
    "    writeFeaturesToCSV(MY_NEGATIVES_FILE_NAME, features)\n",
    "\n",
    "    #Acquires the negative features and writes them to the file.\n",
    "    rootpath = \"dataset/Non_Sign\"\n",
    "    features = calculateHog(rootpath)\n",
    "    writeFeaturesToCSV(NON_SIGN_FEATURES_FILE_NAME, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFeaturesAndWrite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFeatures():\n",
    "    sign_features = readCSV(SIGN_FEATURES_FILE_NAME)\n",
    "    non_sign_features = readCSV(NON_SIGN_FEATURES_FILE_NAME)\n",
    "    my_non_sign_features = readCSV(MY_NEGATIVES_FILE_NAME)\n",
    "    video_non_sign_features = readCSV(\"video_negatives.csv\")\n",
    "\n",
    "    non_sign_features = np.concatenate((non_sign_features, my_non_sign_features), axis=0)\n",
    "    non_sign_features = np.concatenate((non_sign_features, video_non_sign_features), axis=0)\n",
    "\n",
    "    return (sign_features, non_sign_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sign_features, non_sign_features) = loadFeatures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareTestAndTrain(sign_features, non_sign_features):\n",
    "    features = np.concatenate((sign_features, non_sign_features), axis=0)\n",
    "    labels = np.append(np.ones(len(sign_features)), np.zeros(len(non_sign_features)))\\\n",
    "                .reshape(len(sign_features) + len(non_sign_features), 1)\n",
    "\n",
    "    data = np.hstack((features, labels))\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    percentage = 80\n",
    "    partition = int(len(data) * percentage / 100)\n",
    "\n",
    "    f_train, f_test = data[:partition, :-1], data[partition:, :-1]\n",
    "    l_train, l_test = data[:partition, -1].ravel(), data[partition:, -1].ravel()\n",
    "\n",
    "    f_train = f_train.astype('float32')\n",
    "    f_test = f_test.astype('float32')\n",
    "    l_train = l_train.astype('int32')\n",
    "    l_test = l_test.astype('int32')\n",
    "    \n",
    "    return (f_train, f_test, l_train, l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure and return linear SVM.\n",
    "#max_iter - maximum number of training iterations\n",
    "#precision - target precision\n",
    "def configLinearSvm(max_iter, precision):\n",
    "    svm = cv2.ml.SVM_create()\n",
    "    svm.setType(cv2.ml.SVM_C_SVC)\n",
    "    svm.setKernel(cv2.ml.SVM_LINEAR)\n",
    "    svm.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER, max_iter, precision))\n",
    "    \n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveSvm(svm, filename):\n",
    "    svm.save(filename)#svmmodel.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSvm(svm):\n",
    "    (f_train, f_test, l_train, l_test) = prepareTestAndTrain(sign_features, non_sign_features)\n",
    "    svm.train(f_train, cv2.ml.ROW_SAMPLE, l_train)\n",
    "    \n",
    "    retval, l_pred = svm.predict(f_test)\n",
    "\n",
    "    print(classification_report(l_test, l_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = configLinearSvm(100000, 1e-6)\n",
    "trainSvm(svm)\n",
    "saveSvm(svm, \"model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dealocate arrays\n",
    "sign_features = None\n",
    "non_sign_features = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "        \n",
    "    pick = []\n",
    "\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    "\n",
    "    while len(idxs) > 0:\n",
    "\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    "\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    "\n",
    "        idxs = np.delete(idxs, np.concatenate(([last], np.where(overlap > overlapThresh)[0])))\n",
    "\n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = cv2.ml.SVM_load(\"model.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTrafficSigns(image, svm, hog, round_num, it, features, save = False):\n",
    "    fd = hog.compute(image, winStride=(16, 16))\n",
    "    \n",
    "    matches = []\n",
    "    for i in range(0, len(fd), 1764):\n",
    "        arr = fd[i:i + 1764]\n",
    "        retval, pred = svm.predict(arr.T)\n",
    "        if pred[0] == 1:\n",
    "            x = int(((i / 1764) % 125) * 16)\n",
    "            y = int(((i / 1764) // 125) * 16)\n",
    "            matches.append((x, y))\n",
    "            \n",
    "            window = image[y:y + 64, x:x + 64]\n",
    "            if save:\n",
    "                cv2.imwrite(\"dataset/autogen/v_iter\" + str(round_num) + \"_\" + str(it) + \".jpg\", window)\n",
    "            \n",
    "            features.append(np.ravel(arr).tolist())\n",
    "            \n",
    "            it = it + 1\n",
    "            \n",
    "\n",
    "    for m in matches:\n",
    "        cv2.rectangle(image, (m[0], m[1]), (m[0] + 64, m[1] + 64), (255, 0, 0), 2)\n",
    "        \n",
    "    return image, it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('frankfurt.mp4')\n",
    "\n",
    "if cap.isOpened() == False:\n",
    "    print(\"Error opening video file.\")\n",
    "\n",
    "round_num = 3\n",
    "it = 0\n",
    "\n",
    "hog = cv2.HOGDescriptor((64, 64), (16, 16), (8, 8), (8, 8), 9)\n",
    "\n",
    "cap.set(1, 8500)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "j = 0\n",
    "while ret and  j < 100:\n",
    "    \n",
    "    features = []\n",
    "\n",
    "    resized = cv2.resize(frame, (2048, 1024))\n",
    "    (image, it) = findTrafficSigns(resized, svm, hog, round_num, it, features, save = False)\n",
    "    #appendFeaturesToCSV(\"video_negatives.csv\", features)\n",
    "\n",
    "    cv2.imshow('Video', image)\n",
    "\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "        \n",
    "    j = j + 1\n",
    "        \n",
    "    ret, frame = cap.read()\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainCNN(rootpath):\n",
    "    '''Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "    Returns:   list of images, list of corresponding labels'''\n",
    "    images = [] # images\n",
    "    labels = [] # corresponding labels\n",
    "    # loop over all 42 classes\n",
    "    for c in range(0,43):\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/' # subdirectory for class\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv') # annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "        next(gtReader, None) # skip header\n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            images.append(cv2.resize(cv2.imread(prefix + row[0]), (64, 64))) # the 1th column is the filename\n",
    "            labels.append(int(row[7])) # the 8th column is the label\n",
    "        gtFile.close()\n",
    "    return images, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTestCNN(rootpath):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    gtFile = open(rootpath + \"/GT-final_test.csv\") # annotations file\n",
    "    gtReader = csv.reader(gtFile, delimiter=';') # csv parser for annotations file\n",
    "    next(gtReader, None) # skip header\n",
    "    \n",
    "    for row in gtReader:\n",
    "        images.append(cv2.resize(cv2.imread(rootpath + '/' + row[0]), (64, 64))) # the 1th column is the filename\n",
    "        labels.append(int(row[7])) # the 8th column is the label\n",
    "    gtFile.close()\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPool2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5:\n",
    "    def __init__(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Conv2D(24, 5, input_shape=(64, 64, 3), padding='same', activation='relu'))\n",
    "        self.model.add(Dropout(0.65))\n",
    "        self.model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        self.model.add(Conv2D(64, 5, padding='same', activation='relu'))\n",
    "        self.model.add(Dropout(0.65))\n",
    "        self.model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(480, activation='relu'))\n",
    "        self.model.add(Dropout(0.67))\n",
    "        self.model.add(Dense(84, activation='relu'))\n",
    "        self.model.add(Dropout(0.67))\n",
    "        self.model.add(Dense(43, activation='softmax'))\n",
    "        \n",
    "    def compile(self):\n",
    "        self.model.compile(optimizer=SGD(lr=0.0005),\\\n",
    "                           loss=\"categorical_crossentropy\",\\\n",
    "                           metrics=[\"accuracy\"])\n",
    "    \n",
    "    def train(self, trainData, trainLabels, epochs_num):\n",
    "        return self.model.fit(trainData, trainLabels, batch_size=32, epochs=epochs_num, verbose=1)\n",
    "        \n",
    "    def evaluate(self, testData, testLabels):\n",
    "        (loss, accuracy) = self.model.evaluate(testData, testLabels, batch_size=32, verbose=1)\n",
    "        print(\"accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, trainLabels = readTrainCNN(\"dataset/GTSRB_Final_Training_Images/GTSRB/Final_Training/Images\")\n",
    "testData, testLabels = readTestCNN(\"dataset/GTSRB_Final_Test_Images/GTSRB/Final_Test/Images\")\n",
    "\n",
    "valData = testData[int(len(testData) * 0.6):len(testData)]\n",
    "valLabels = testLabels[int(len(testLabels) * 0.6):len(testLabels)]\n",
    "\n",
    "testData = testData[0:int(len(testData) * 0.6)]\n",
    "testLabels = testLabels[0:int(len(testLabels) * 0.6)]\n",
    "\n",
    "trainData = np.array(trainData)\n",
    "testData = np.array(testData)\n",
    "valData = np.array(valData)\n",
    "\n",
    "trainData = trainData.astype('float32') / 255.0\n",
    "testData = testData.astype('float32') / 255.0\n",
    "valData = valData.astype('float32') / 255.0\n",
    "\n",
    "trainLabels = np_utils.to_categorical(trainLabels, 43)\n",
    "testLabels = np_utils.to_categorical(testLabels, 43)\n",
    "valLabels = np_utils.to_categorical(valLabels, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPerm = np.random.permutation(len(trainData))\n",
    "trainData = trainData[trainPerm]\n",
    "trainLabels = trainLabels[trainPerm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = LeNet5()\n",
    "cnn.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.train(trainData, trainLabels, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.evaluate(testData, testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.model.save(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded = load_model(\"cnn12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = cv2.resize(cv2.imread(\"C:/Users/dlalic/Desktop/testright.png\"), (64, 64))\n",
    "\n",
    "cv2.imshow(\"Sign\", l)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "temp_list = [l]\n",
    "npl = np.array(temp_list)\n",
    "print(npl.shape)\n",
    "\n",
    "pred = loaded.predict(npl)\n",
    "\n",
    "print(pred)\n",
    "print(np.argmax(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
